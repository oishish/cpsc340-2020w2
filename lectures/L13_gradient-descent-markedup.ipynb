{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 13: gradient descent\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=ao34xqQvuT4&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n",
    "1. Easily by Bruno Major\n",
    "2. The Archer by Taylor Swift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a3 in progress\n",
    "- My office hour tomorrow moved from 12pm to 12:30pm (one time only)\n",
    "- Countdown to reading week: 3 more classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- preamble\n",
    "- motivation\n",
    "- gradient descent definition\n",
    "- normal equations vs. gradient descent\n",
    "- least squares demo\n",
    "- minimizing the absolute value loss\n",
    "- uses of gradient descent\n",
    "- the 3 decisions\n",
    "- data space vs. parameter space\n",
    "- Huber loss\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2016W2 final\n",
    "\n",
    "Assuming we want to use standard gradient descent as our optimizer, what is a strategy for dealing with non-differentiable points in a loss function (for example, an absolute value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a slightly modified loss function without these 'kinks'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2017S1 midterm\n",
    "\n",
    "Why do we need gradient descent for robust linear regression but not for least squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Least squares has a unique minimizer that can be determined by the _Normal Equations_.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it useful to know whether or not an objective function is convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "A non-convex function cannot be easily minimized just by using gradient descent since a non-convex function can have local minima.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2017W2 midterm\n",
    "\n",
    "Under what circumstances might a gradient descent update result in the loss increasing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "If you 'overstep' the minima during a step, then the gradient descent update will cause an increase in the loss. e.g. if $\\alpha$ is very large.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2018W1 midterm\n",
    "\n",
    "Name one advantage and one disadvantage of using gradient descent instead of the normal equations to fit a least squares linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. Advantage: Can approximate a solution quickly.\n",
    "    \n",
    "2. Disadvantage: Usually slower for most cases.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/239570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. Before starting gradient descent, we don't know how many steps we'll need to reach a desired level of accuracy.\n",
    "2. To use gradient descent, we need to apply the gradient operator once, but evaluate the gradient function many times.\n",
    "3. The main disadvantage of setting the learning rate, $\\alpha$, too small is that convergence will be very slow.\n",
    "4. The main disadvantage of setting the learning rate, $\\alpha$, too large is that convergence will be very slow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. True (ish) but we can estimate how long it will take.\n",
    "\n",
    "2. True (hopefully not _too_ many times).\n",
    "    \n",
    "3. True.\n",
    "    \n",
    "4. False.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In supervised learning, every point in the parameter space corresponds to a regression/classification surface in the data space.\n",
    "2. In general, dimensionality of the data space $\\geq$ dimensionality of the parameter space.\n",
    "3. Each iteration of gradient descent moves you to a new point in parameter space.\n",
    "4. The loss function defines a score for each point in data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. True.\n",
    "    \n",
    "2. False. You can have more parameters than number of features.\n",
    "\n",
    "3. True.\n",
    "   \n",
    "4. False.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

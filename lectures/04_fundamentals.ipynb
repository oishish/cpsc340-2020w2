{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 4: Fundaments of Learning\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=dPm-KTrJlFU&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a1 due Wednesday night\n",
    "- Tutorials started this week\n",
    "- Office hours started last week\n",
    "- Q&A on [Ed](https://edstem.org/us/courses/3226/)\n",
    "- Note that this lecture overlaps a lot with CPSC 330\n",
    "  - But with a difference in terminology: CPSC 330's \"deployment\" is called \"test\" here.\n",
    "- Recordings of these sessions are available in the Zoom section of Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture chapters\n",
    "\n",
    "- notation review\n",
    "- digits dataset\n",
    "- decision tree demo\n",
    "- error vs. tree depth\n",
    "- overfitting\n",
    "- train/test notation\n",
    "- Golden Rule of ML\n",
    "- iid assumption\n",
    "- the fundamental tradeoff\n",
    "- validation error\n",
    "- parameters vs. hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. More complicated models, like deeper decision trees, tend to achieve a lower training error (than simpler models). \n",
    "2. More complicated models, like deeper decision trees, tend to achieve a lower approximation error (than simpler models). \n",
    "3. More complicated models, like deeper decision trees, tend to achieve a lower test error (than simpler models). \n",
    "4. More complicated models, like deeper decision trees, tend to require more data to train (than simpler models). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers**:\n",
    "\n",
    "1. True\n",
    "2. False\n",
    "3. We don't know what will happen to the test error in general, so False\n",
    "4. True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting or underfitting questions\n",
    "\n",
    "Definitions:\n",
    "\n",
    "- Underfitting: your model is too simple such that your training error is high.\n",
    "- Overfitting: your model is too complicated such that your approximation error is high.\n",
    "\n",
    "For each of the following, would you associate the situation more with underfitting or overfitting?\n",
    "\n",
    "1. You meet a cyclist wearing a helmet. You conclude that \"all cyclists wear helmets, and all pedestrians don't wear helmets\".\n",
    "2. You meet 1000 cyclists and all are wearing helmets. You conclude that \"all cyclists wear helmets, and all pedestrians don't wear helmets\".\n",
    "3. Of your 4 instructors this term, your CPSC 340 instructor is the most disorganized. You conclude that \"any UBC instructor wearing a grey shirt, black headphones or with brown hair and brown eyes must be disorganized.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**:\n",
    "\n",
    "1. Following the in-class discussion, this is a bit ambiguous. The model is very simple (which made me think of underfitting), but it's also getting 0 training error which is a sign of overfitting. See if I can tease this apart into a question with a clearer answer.\n",
    "2. As above.\n",
    "3. Overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and underfitting questions, Attempt #2\n",
    "\n",
    "1. You want to predict whether a person is wearing contact lenses. You think about your best friends, who are a mix of people with glasses, people with contact lenses, and people with perfect vision. You conclude, \"People with glasses don't wear contact lenses, but everyone else wears contact lenses\". \n",
    "\n",
    "Commentary: This hopefully now a clearer example of underfitting. The model is extremely simple; in fact too simple to even do well on your training data, because it's wrong about all your friends with perfect vision. However, the model is not silly or wrong - it makes sense that people with glasses probably don't wear contact lenses. In terms of a decision tree, saying \"if YES glasses then predict NO contact lenses\" is not that unreasonable. But the other half, \"if NO glasses then predict YES contact lenses\" is too simple, and would need further splitting on other features (e.g. age) to be a better model.\n",
    "\n",
    "2. You want to predict whether a person is wearing contact lenses. You think about all your friends. You come up with a model that perfectly describes your personal situation, \"People with glasses don't wear contact lenses. For people _without_ glasses, only the following groups wear contact lenses: those between the ages of 20-22 and 30-35, or people who like sports and ice cream\". \n",
    "\n",
    "Commentary: This is hopefully a clear example of overfitting. The model perfectly describes your personal situation (i.e. 100% training accuracy) but the complexity is misused to make something too specific to your personal situation. Yes, it may be the case that in your small friend group the people who wear contact lenses are aged 21 and 32, but this probably doesn't generalize to the whole population. There are some grains of truth here (glasses, maybe sports) but also some nonsensical aspects of the model (ice cream). Without decision tree thinking, this takes the initial underfitting tree (too simple), but makes crazy splits based on your particular training set. \n",
    "\n",
    "Now imagine you polled 1000 people instead of just your 10 friends. In that case, predicting that people who like ice cream wear contact lenses would probably not get you a good score. Hence, more training data _combats overfitting_. Also, imagine you tried out your overfitting model on your cousins who were not in the \"training set\". The model will probably not perform well on them - there's no reason to think those who like ice cream would have contact lenses. Thus, a validation set _diagnoses overfitting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion question\n",
    "\n",
    "I fit a decision tree to a dataset. I obtain a training error of 0.1 and a validation error of 0.2. Then, I realize I had violated the Golden Rule of ML - some of this same validation data had actually been used for training!  \n",
    "\n",
    "1. Would you say the training error of 0.1 is \"wrong\" or misleading? If so, is it too low or too high?\n",
    "2. Would you say the validation error of 0.2 is \"wrong\" or misleading? If so, is it too low or too high?\n",
    "3. What do you think the test error might be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers**:\n",
    "\n",
    "1. I would say this is fine.\n",
    "2. Yes, this is misleading - the score is inflated (so the error I'm seeing is too low). \n",
    "3. The test error is probably > 0.2; validation error is not a good representation of test error here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

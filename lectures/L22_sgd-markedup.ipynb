{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 22: stochastic gradient descent\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=lmqV5Z5HZzc&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=22)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 夜に駆ける by YOASOBI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 solution posted\n",
    "- a5 posted\n",
    "- Final exam scheduling survey: https://ubc.ca1.qualtrics.com/jfe/form/SV_0cGkvmKdr8Aq6SG\n",
    "- Bug fixed from last class, see https://edstem.org/us/courses/3226/discussion/306056\n",
    "- Tutorial was missed this morning, rescheduled for 5pm today (right after class)\n",
    "  - See Canavas for Zoom link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- kernel trick recap\n",
    "- stochastic gradient motivation\n",
    "- SGD vs. GD\n",
    "- minimizing averages\n",
    "- per-example gradients\n",
    "- visualizing SGD: 1 parameter\n",
    "- decreasing step sizes\n",
    "- stochastic average gradient\n",
    "- visualizing SGD: 2 parameters\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap the different decisions you make:\n",
    "\n",
    "1. Pick a model <-- affects overfitting \n",
    "2. Pick a loss <-- affects overfitting (e.g. regularization)\n",
    "3. Pick an optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch -> # of iterations to look through whole dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. For stochastic gradient to work, we need to increase the step size as the optimization proceeds.\n",
    "2. Stochastic gradient can be used for training many models, including linear regression and logistic regression.\n",
    "3. An iteration of stochastic gradient might cause the loss might go up, even for a very small learning rate.\n",
    "4. It is reasonable to check whether the loss went up or down after each iteration of stochastic gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "1. **False**. Need to decrease step size\n",
    "2. **True**. Why not?\n",
    "3. **True**. Definitely, lots of randomness\n",
    "4. **False**. Probably a waste of time, maybe wait some $n$ iterations?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk about minibatches, iterations, and epochs\n",
    "\n",
    "- SGD as defined in the video looks at 1 example at a time\n",
    "- But you could look at more than one example at a time\n",
    "- The number of examples you look at (per iteration) is called the _minibatch size_ or _batch size_\n",
    "- An _iteration_ is one update to $w$\n",
    "  - In regular gradient descent, you look at $n$ examples per iteration\n",
    "  - In SGD, you look at $k$ examples per iteration, where $k$ is the minibatch size\n",
    "- An _epoch_ is the number of iterations it takes to pass through the whole training set.\n",
    "  - If minibatch size is $1$, then you have $n$ iterations per epoch\n",
    "  - If minibatch size is $10$, then you have $n/10$ iterations per epoch\n",
    "- Why am I bothering you with this?\n",
    "  - If you tell me you did 1000 iterations of SGD, I don't know if that's a lot or a little; it really depends on your minibatch size.\n",
    "  - If you tell me you did 1000 epochs of SGD, then I have an idea that this is quite a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More T/F questions\n",
    "\n",
    "These questions pertain to the idea of batch size (aka minibatch size) and epochs:\n",
    "\n",
    "1. Increasing the batch size results in slower, but better, iterations.\n",
    "2. If you double the number of epochs and double the batch size, then nothing has changed.\n",
    "3. Increasing the batch size makes one epoch slower.\n",
    "4. One epoch of stochastic gradient takes about the same amount of time as one iteration of gradient descent.\n",
    "5. Stochastic gradient with a minibatch size of $n$ is the same thing as gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "1. **True**.\n",
    "2. **False**. Same computational time ish but different $w$ iterations.\n",
    "3. **False**. Epoch time ind. of batch size.\n",
    "4. **True**. Roughly.\n",
    "5. **True**.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n",
    "\n",
    "#### 2016W2 final\n",
    "\n",
    "Name an advantage and a disadvantage of using a smaller minibatch size in stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "**Advantage**: Quicker iterations on $w$ if number of dimensions is very large.\n",
    "\n",
    "**Disadvantage**: More iterations necessary to reach minima. Less accessible to parallelization.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018W1 final\n",
    "\n",
    "![](img/L22/parta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "$$ O(d) + O(d) $$\n",
    "If you are smart about it, you need only update one value in $w$. You will need to calculate the vector dot product of $g$ with $x_i$ which will be dependent on the number of dimensions. Assuming a batch size of 1.\n",
    "\n",
    "    \n",
    "Right for the wrong reasons. Need to consider the cost of updating $w$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/L22/partb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Still $O(d)$ cheaper $g$ calculation does not help with the vector dot product calculation.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 20: linear classifiers: multi-class\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=x-TU-0PMm-Q&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=21)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unravel by Shape / Shift\n",
    "2. Snowman by Sia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 due March 10 (Wed next week)\n",
    "- Office hours\n",
    "  - Often empty according to TAs\n",
    "  - A great way to get extra help\n",
    "  - It's fine to ask about material from a while ago if you're behind\n",
    "- Next class, Monday March 8:\n",
    "  - I am unable to attend class due to an immovable family commitment.\n",
    "  - Regular class will be replaced with a bonus lecture by Daniele Reda (TA) on introduction to reinforcement learning.\n",
    "  - Attendance is optional. \n",
    "- I forgot to put a4 in Gradescope (again), will do very soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class student Q\n",
    "\n",
    "Why do the terms in the loss not cancel?\n",
    "\n",
    "$$f(w) = \\sum_{i=1}^n \\log(1+\\exp(-y_iw^Tx_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the past it didn't matter much how we encode $y$. \n",
    "- For linear classifiers we specifically require that $y_i \\in \\{-1,+1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if $w^Tx_i>0$ you predict $+1$\n",
    "- if $w^Tx_i<0$ you predict $-1$\n",
    "\n",
    "therefore\n",
    "\n",
    "- if $y_iw^Tx_i>0 $ you were correct \n",
    "- if $y_iw^Tx_i< 0$ you were incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: the statistics people encode $y_i \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another warning: isn't it weird that with $k$ classes we have $k$ weight vectors and $k$ intercepts, EXCEPT when $k=2$ we only have **one** weight vector and **one** intercept. \n",
    "\n",
    "Again, the stats people do things a bit differently here. They actually have $k-1$ weight vectors/intercepts for $k$-classes, which is more coherent with the binary classification case. But in ML we do it our way and it'll become more clear when we get to neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- review\n",
    "- max margin\n",
    "- motivation: POS tagging\n",
    "- one-vs-rest aka one-vs-all\n",
    "- shape of decision boundaries\n",
    "- softmax function\n",
    "- softmax loss\n",
    "- Frobenius norm\n",
    "- Q&A \n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. If your dataset is linearly separable that means you can get zero training error with logistic regression.\n",
    "2. If your dataset is linearly separable that means you can get zero training loss with logistic regression.\n",
    "3. With a multi-class linear classifier, we have one intercept _per class_ (note: this may not have been mentioned in the video?).\n",
    "4. The one-vs-all approach involves solving one optimization problem of higher dimension; the softmax approach involves solving many optimization problems each of lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. **True.**\n",
    "2. **False.** I think you can never get to zero loss on logistic regression. \n",
    "3. **True.** We must, since we have a weight for each class coming out in the end.\n",
    "4. **False.** It the reverse, dimension of the softmax problem is $kc$ where $c$ is the # of classes.\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapted from 2016W2 final\n",
    "\n",
    "For each of the models below, how many parameters does the model have? Your answer may depend on $n$, $d$, and perhaps additional quantities (if so, you need to define them). \n",
    "\n",
    "- (a) Logistic regression for binary classification.\n",
    "\n",
    "- (b) Multi-class logistic regression using softmax.\n",
    "\n",
    "- (c) Multi-class logistic regression using one-vs-all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W1 final\n",
    "\n",
    "![](img/L20/predict.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** At some point we changed the shape of $W$ from $d\\times k$ to $k\\times d$. Luckily this change was made before the 2017W2 videos were filmed. However, this question below is from 2016W1 and uses the old convention. We would write this as\n",
    "\n",
    "$$W = \\begin{bmatrix}-1 &+2\\\\+2 &0 \\\\ 0 &+3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "(a) Prediction would be $y = 3$. It corresponds to the largest weight.\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add-on to the above\n",
    "\n",
    "Consider training a one-vs-rest logistic regression on the dataset above. For the first of the 3 classifiers, what would the training data look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "```\n",
    "y = [+1]\n",
    "    [+1]\n",
    "    [+1]\n",
    "    [-1]\n",
    "    [-1]\n",
    "    [-1]\n",
    "    [-1]\n",
    "    [-1]\n",
    "    [-1]\n",
    "    [-1]\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017W1 final\n",
    "\n",
    "What is the key difference between “one vs. all” logistic regression and training using the softmax loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 26: optional review / more PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Last Of The Red Hot Fools by The Jitters\n",
    "2. Are You All Good? by breathe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- [Ed post on kernel k-means](https://edstem.org/us/courses/3226/discussion/327280)\n",
    "- Summer TA positions available (including CPSC 340 in summer term 1): http://cs.ubc.ca/ta\n",
    "- a5 opened for submissions of Gradescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017W1 final\n",
    "\n",
    "What are two reasons that the minimizer $W$ of the PCA objective is not unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "1. For a given \"best $n$D hyperplane\", there are infinite # of sets of basis vectors that can construct that hyperplane.\n",
    "\n",
    "2. More specifically, we can consider just the -ve of any of those sets to produce another set.\n",
    "    \n",
    "3. Other transformations $\\to$ scaling, rotations, label switching.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that PCA and $k$-means are trying to optimize the same objective function (sum of squared errors). If we could find the optimal $k$-means clustering, why would it have a higher value of this objective function than PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "PCA allows for non-binary contributions from each basis vector. This means we can _automatically_ do a better job approximating any given data point.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. It's best to pick $k$ as large as possible so that we capture most of the variance in the data.\n",
    "2. The first principal component is always the one with highest variance in the data.\n",
    "3. If your $X$ has two identical columns, then the variance explained by your last (i.e. $d$th) principal component will be zero.\n",
    "4. If you $X$ has two identical rows, then your $Z$ must also have two identical rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "1. **False**. Larger $k$ means distilling the data less. Yes, you capture more variance, but in the limit $k \\to d$, what was the point?\n",
    "    \n",
    "2. **True**. By design of PCA (very conveniently).\n",
    "    \n",
    "3. **True**. This is basically just saying that we have a duplicate label. Therefore the PCA should be able to fully explain all variance in $d - 1$ principal components. \n",
    "    \n",
    "4. **True**. Can't transform the same point differently with the same transformation.\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Q&A\n",
    "\n",
    "About PCA, earlier material, other machine learning, life..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

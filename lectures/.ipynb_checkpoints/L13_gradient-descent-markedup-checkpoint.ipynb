{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 13: gradient descent\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=ao34xqQvuT4&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n",
    "1. Easily by Bruno Major\n",
    "2. The Archer by Taylor Swift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a3 in progress\n",
    "- My office hour tomorrow moved from 12pm to 12:30pm (one time only)\n",
    "- Countdown to reading week: 3 more classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- preamble\n",
    "- motivation\n",
    "- gradient descent definition\n",
    "- normal equations vs. gradient descent\n",
    "- least squares demo\n",
    "- minimizing the absolute value loss\n",
    "- uses of gradient descent\n",
    "- the 3 decisions\n",
    "- data space vs. parameter space\n",
    "- Huber loss\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2016W2 final\n",
    "\n",
    "Assuming we want to use standard gradient descent as our optimizer, what is a strategy for dealing with non-differentiable points in a loss function (for example, an absolute value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** use a smooth approximation to the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2017S1 midterm\n",
    "\n",
    "Why do we need gradient descent for robust linear regression but not for least squares?\n",
    "\n",
    "**Mike's answer:** There's no closed form solution for $w$ once you move away from least squares. Least squares linear regression is a special case where the gradient is a linear function of $w$ - pretty much everything else needs some sort of iterative optimization method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it useful to know whether or not an objective function is convex?\n",
    "\n",
    "**Mike's answer:** No local minima. If you find a place where the gradient is 0, you be confident you minimized the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2017W2 midterm\n",
    "\n",
    "Under what circumstances might a gradient descent update result in the loss increasing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** $\\alpha$ very large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2018W1 midterm\n",
    "\n",
    "Name one advantage and one disadvantage of using gradient descent instead of the normal equations to fit a least squares linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** \n",
    "\n",
    "- Advantage: you can control the tradeoff between time spent vs. accuracy; may converge faster for very large datasets.\n",
    "- Disadvantage: probably be slower (and more annoying) in most cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/239570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. Before starting gradient descent, we don't know how many steps we'll need to reach a desired level of accuracy.\n",
    "2. To use gradient descent, we need to apply the gradient operator once, but evaluate the gradient function many times.\n",
    "3. The main disadvantage of setting the learning rate, $\\alpha$, too small is that convergence will be very slow.\n",
    "4. The main disadvantage of setting the learning rate, $\\alpha$, too large is that convergence will be very slow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "1. True. \n",
    "2. True. \n",
    "3. True. \n",
    "4. False. The optimization might diverge and never converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In supervised learning, every point in the parameter space corresponds to a regression/classification surface in the data space.\n",
    "2. In general, dimensionality of the data space $\\geq$ dimensionality of the parameter space.\n",
    "3. Each iteration of gradient descent moves you to a new point in parameter space.\n",
    "4. The loss function defines a score for each point in data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "1. True. \n",
    "2. False. More on this next class with polynomial regression.\n",
    "3. True. \n",
    "4. False. -> The loss function defines a score for each point in **parameter** space. It's $f(w)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

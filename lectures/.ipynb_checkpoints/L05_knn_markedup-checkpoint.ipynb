{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 5: Fundaments of Learning continued + KNN\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=JRF6oELLn0M&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a1 due tonight\n",
    "  - **The submission process takes a bit of time with labelling your pages, so start this process before 11pm!!!**\n",
    "- Tutorials started this week\n",
    "- Office hours started last week\n",
    "- Q&A on [Ed](https://edstem.org/us/courses/3226/)\n",
    "- **Bonus slides** now posted on the course schedule.\n",
    "  - These are 100% optional and just if you're interested.\n",
    "- The [course Spotify playlist](https://open.spotify.com/playlist/7HaaNdz7qkReXYwxaHQjGz?si=EPzioWciSNyaWIxpwK6Srw) is now complete.\n",
    "  - There were 2 repeats, and one that I had to... censor.\n",
    "  - Looking forward to the Pokemon theme song! üòà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- fundamentals recap\n",
    "- optimization bias\n",
    "- cross-validation\n",
    "- no free lunch theorem\n",
    "- $k$-nearest neighbours\n",
    "- curse of dimensionality\n",
    "- $k$-NN implementation \n",
    "- parametric vs. non-parametric\n",
    "- norms\n",
    "- decision boundary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions megathread\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/212834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W2 midterm:\n",
    "\n",
    "Consider the following two approaches to validating a model:\n",
    "\n",
    "- Split my data in half. The first half is used for training and the second half for validation.\n",
    "- 2-fold cross validation.\n",
    "\n",
    "Briefly explain the difference between the two approaches, and name one advantage of each\n",
    "approach. Note: you can assume the data set was randomly shuffled in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: in the first case we train one model and assess/score once. In the second case we train two models and score twice, then average the scores. Advantage of 1st approach: 2x faster; advantage of CV: more accurate score measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017S1 midterm:\n",
    "\n",
    "You have a (fictional) model that trains in $O(n^2d)$ time (total) and makes predictions in $O(d^2)$ time (per prediction). What is the time complexity of evaluating this model with $k$-fold cross-validation? Answer using big-O notation and explain or show your work. Your answer may depend on $n$, $d$, and/or $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: \n",
    "\n",
    "Amount of data in prediction: $n/k$\n",
    "\n",
    "Amount of data for training: $n - n/k = n(1-1/k) = n(k-1)/k$\n",
    "\n",
    "Training:\n",
    "\n",
    "$((k-1)/k \\times n)^2 d \\times k  \\approx n^2 dk$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction/scoring/testing: \n",
    "\n",
    "$n/k \\times d^2 \\times k = nd^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total time should be something like $O(n^2dk + nd^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2018W1 midterm:\n",
    "\n",
    "- Consider the ‚Äúconsistent nearest neighbour‚Äù classifier: it runs our usual KNN classifier but instead of viewing $k$ as a hyper-parameter it always sets $k = ‚åà\\log(n)‚åâ$ (the logarithm of $n$ rounded up to the nearest integer). Would call this a parametric classifier or a non-parametric classifier? Briefly justify your answer\n",
    "\n",
    "- Consider the \"condensed nearest neighbour\" classifier: at training time it chooses the $c$ \"best\" training examples (where $c$ is a hyper-parameter), and at test time uses the usual KNN prediction but based only on these $c$ training examples. Would call this a parametric classifier or a non-parametric classifier? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: \n",
    "\n",
    "- I would call this a non-parametric classifier. This is basically the same as regular KNN but for some special value of $k$. You still need to keep the whole training set around when making predictions.\n",
    "\n",
    "- I would call this one parametric. You don't need to keep the whole training set around anymore. Prediction time is now independent of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also from 2018W1 midterm:\n",
    "\n",
    "Consider the binary classification training data set shown below. The two classes are denoted with o‚Äôs and √ó‚Äôs.\n",
    "\n",
    "![](img/L2/2018W1mt.png)\n",
    "\n",
    "What is the maximum value of $k$ such that KNN gets zero training error on this data set? Assume ties are broken by voting for class +1 (the circles). Briefly explain your reasoning. Feel free to draw on the plot if it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: $k=1$. Even with $k=2$ you already get training errors, e.g. the blue x in the middle, it's 2 NNs are itself and the nearby red dot, and by the tie-breaking procedure you'd predict red which is incorrect. Same sort of thing happens for $k=3,...$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

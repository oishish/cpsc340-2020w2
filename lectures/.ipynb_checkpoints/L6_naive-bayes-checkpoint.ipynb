{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 6: Naive Bayes\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=sUtPiyMnkIU&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a2 posted\n",
    "- Last class I forgot to mention I updated the Lecture 4 document with some new examples\n",
    "- Office hours queue: https://queue.students.cs.ubc.ca/course/17/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey\n",
    "\n",
    "https://ubc.ca1.qualtrics.com/jfe/form/SV_6YkzonFPmnDsD30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- k-NN recap\n",
    "- digit classification\n",
    "- data augmentation\n",
    "- bag of words\n",
    "- notation review\n",
    "- probabilistic classifiers\n",
    "- Bayesian classification\n",
    "- naive Bayes \n",
    "- Laplace smoothing\n",
    "- decision theory\n",
    "- decision trees vs. naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions megathread\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/214758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. In supervised learning, sometimes making up fake training data can be extremely useful.\n",
    "2. The \"Bag of Words\" representation of text disregards the ordering of words in a document.\n",
    "3. The naive Bayes assumption means we assume p(\"hello\"=1,\"vicodin\"=1,\"340\"=1) = p(\"hello\"=1)p(\"vicodin\"=1)p(\"340\"=1).\n",
    "4. Naive Bayes is limited to binary classification (that is, 2 possible values of y, like \"spam\" and \"not spam\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. **True.** cf translational and rotational invariance in identifying handwritten numbers.\n",
    "2. **True.** This is the whole conceit of BOW. Apparently it still works quite well in some cases though:)\n",
    "3. **False.** The naÃ¯ve Bayes assumption assume _conditional_ independence. This is the definition of regular ind.\n",
    "4. **False.** We basically said `if p[spam|x] > p[!spam|x] then return spam`. We could have easily actually used the numbers we had calculated.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W2 midterm\n",
    "\n",
    "What is the space complexity of a trained naive Bayes model with binary features and binary labels? Answer using big-O notation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "$O(d)$: With $d$ being the number of features. Since they are binary it helps us somewhat.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017W2 midterm\n",
    "\n",
    "What can go wrong is Laplace smoothing is not used with naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Basically without laplace smoothing you can often end up with probabilities of zero because of a single feature which is assumed to _never_ exist.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions I just made up\n",
    "\n",
    "In naive Bayes, why is p(hello)p(vicodin)p(340) easier to approximate than p(hello,vicodin,340)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Storage is only $O(d)$ with $d = $ # of features. Whereas if we wished to store every combination it would be like $O(2^d)$ ish.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time permitting\n",
    "\n",
    "We can look at the \"naive Bayes by hand\" question in a2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

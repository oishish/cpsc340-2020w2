{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 5: Fundaments of Learning continued + KNN\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=JRF6oELLn0M&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a1 due tonight\n",
    "  - **The submission process takes a bit of time with labelling your pages, so start this process before 11pm!!!**\n",
    "- Tutorials started this week\n",
    "- Office hours started last week\n",
    "- Q&A on [Ed](https://edstem.org/us/courses/3226/)\n",
    "- **Bonus slides** now posted on the course schedule.\n",
    "  - These are 100% optional and just if you're interested.\n",
    "- The [course Spotify playlist](https://open.spotify.com/playlist/7HaaNdz7qkReXYwxaHQjGz?si=EPzioWciSNyaWIxpwK6Srw) is now complete.\n",
    "  - There were 2 repeats, and one that I had to... censor.\n",
    "  - Looking forward to the Pokemon theme song! üòà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- fundamentals recap\n",
    "- optimization bias\n",
    "- cross-validation\n",
    "- no free lunch theorem\n",
    "- $k$-nearest neighbours\n",
    "- curse of dimensionality\n",
    "- $k$-NN implementation \n",
    "- parametric vs. non-parametric\n",
    "- norms\n",
    "- decision boundary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions megathread\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/212834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W2 midterm:\n",
    "\n",
    "Consider the following two approaches to validating a model:\n",
    "\n",
    "- Split my data in half. The first half is used for training and the second half for validation.\n",
    "- 2-fold cross validation.\n",
    "\n",
    "Briefly explain the difference between the two approaches, and name one advantage of each\n",
    "approach. Note: you can assume the data set was randomly shuffled in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017S1 midterm:\n",
    "\n",
    "You have a (fictional) model that trains in $O(n^2d)$ time (total) and makes predictions in $O(d^2)$ time (per prediction). What is the time complexity of evaluating this model with $k$-fold cross-validation? Answer using big-O notation and explain or show your work. Your answer may depend on $n$, $d$, and/or $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2018W1 midterm:\n",
    "\n",
    "- Consider the ‚Äúconsistent nearest neighbour‚Äù classifier: it runs our usual KNN classifier but instead of viewing $k$ as a hyper-parameter it always sets $k = ‚åà\\log(n)‚åâ$ (the logarithm of $n$ rounded up to the nearest integer). Would call this a parametric classifier or a non-parametric classifier? Briefly justify your answer\n",
    "\n",
    "- Consider the \"condensed nearest neighbour\" classifier: at training time it chooses the $c$ \"best\" training examples (where $c$ is a hyper-parameter), and at test time uses the usual KNN prediction but based only on these $c$ training examples. Would call this a parametric classifier or a non-parametric classifier? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also from 2018W1 midterm:\n",
    "\n",
    "Consider the binary classification training data set shown below. The two classes are denoted with o‚Äôs and √ó‚Äôs.\n",
    "\n",
    "![](img/L2/2018W1mt.png)\n",
    "\n",
    "What is the maximum value of $k$ such that KNN gets zero training error on this data set? Assume ties are broken by voting for class +1 (the circles). Briefly explain your reasoning. Feel free to draw on the plot if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 26: sparse matrix factorization\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=ghLOWBlzWyw&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=26)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You Rock My World by Michael Jackson\n",
    "2. Summer Paradise by Simple Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a5 due tonight\n",
    "- a6 will be the last assignment\n",
    "- more info on the final exam coming next week (hopefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- PCA recap\n",
    "- eigenfaces\n",
    "- VQ vs. PCA vs. NMF\n",
    "- non-negative least squares\n",
    "- sparsity and non-negativity\n",
    "- projected gradient\n",
    "- NMF for basketball shots\n",
    "- topic modelling\n",
    "- regularized matrix factorization\n",
    "- sparse matrix factorization\n",
    "- why sparsity? \n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016W1 final\n",
    "\n",
    "![](img/L26/2016W1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:** \n",
    "\n",
    "- (a) The minimum of the loss is not unique; if $W, Z$ is a solution then $-W, -Z$ is also a solution.\n",
    "- (b) No effect; here we can keep making $W$ smaller and $Z$ bigger so the loss doesn't really make sense.\n",
    "- (c) Yes, this time $\\lambda_W$ would have an effect; bigger $\\lambda_W$ would lead to higher training error (and lower approximation error).\n",
    "- (d) Yes, $\\lambda_W$ has an effect now (same as answer to c). As we increase $\\lambda_W$ we will get more zeros in $W$ which will increase training error.\n",
    "\n",
    "What about what we had before, if $W$, $Z$ is a solution what about $W/2, 2Z$. The L0 regularization only cares about the number of zeros (or nonzeros), it doesn't care about the magnitude of the values. So $W$ and $W/2$ are equally good - that is, have the same 0 norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016W2 final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/L26/intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/L26/a.png)\n",
    "![](img/L26/b.png)\n",
    "![](img/L26/c.png)\n",
    "![](img/L26/d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018W1 final\n",
    "\n",
    "For the same $k$, do you expect the resulting loss from running NMF to be lower, higher, or the same as the loss from running PCA on the same data? Briefly justify your answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

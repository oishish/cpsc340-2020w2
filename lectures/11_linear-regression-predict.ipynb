{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 11: linear regression: `predict`\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=PBUIPTDSPQE&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a2 due tonight\n",
    "- Per [Ed post](https://edstem.org/us/courses/3226/discussion/232386), you may be interested in [this data science terminology document](https://ubc-mds.github.io/resources_pages/terminology/).\n",
    "- Amit (TA) will join lecture today and will be available to answer questions / join breakout rooms. We can do a poll at the end of class to see if this is worth continuing.\n",
    "- Amit will also have office hours right after lecture today (and recurring on Wednesdays).\n",
    "- Countdown to reading week: 5 more classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- regression intro\n",
    "- simple linear regression\n",
    "- terminology woes\n",
    "- least squares objective\n",
    "- minimizing squared error in 1d\n",
    "- multiple linear regression\n",
    "- preparations for least squares in d dimensions\n",
    "- the y-intercept\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2016W1 midterm\n",
    "\n",
    "![](img/L11/2016w1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** $(2/3)\\times9 + (-1/4)\\times4=6-1=5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2016W2 midterm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following methods are sensitive to the normalization/scaling of the features? For example, if one of your features is length, which methods would give different results (predictions on test data) if the unit of length is changed from metres to centimetres across all the (train and test) data?\n",
    "\n",
    "- KNN classifier\n",
    "- linear regression\n",
    "- decision tree\n",
    "- random forest\n",
    "- $k$-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** \n",
    "\n",
    "- KNN: yes. You're computing distances between examples. \n",
    "- linear regression: no. Because if you multiple a feature by 10, the weight you learn will go down by 1/10. The predictions will be the same (because you're multiplying a weight that's 1/10 the original size with an x_test that's 10x the original size). However!! this won't be true anymore when we get to regularized linear regression in a couple weeks. \n",
    "- decision tree: no. The thresholds will change but the predictions won't. \n",
    "- random forest: no. Same as decision trees; they are just a bunch of decision trees. \n",
    "- $k$-means: yes. It's based on distances like KNN and scaling will affect the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2017W2 midterm\n",
    "\n",
    "What is the time complexity of predicting on a single test instance with linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** $O(d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also from the 2017W2 midterm\n",
    "\n",
    "![](img/L11/lrprediction.png)\n",
    "\n",
    "Using the above weights $w$, the prediction for the test vector $\\tilde{x}_1$ is $\\hat{y}_1 = 10$. Find the prediction for the test vector $\\tilde{x}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** Going from x1 to x2 we've increased feature 1 by 2 and increased feature 5 by 1. So the prediction is $10 + 2\\times 2 + (-5)\\times 1=9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the 2018W1 midterm\n",
    "\n",
    "Describe a situation where using a linear regression model with the squared error could give very bad predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** The relationship between x and y might not be linear. Extreme values / outliers in the data might make squared error a bad choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Time permitting) Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/231959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 6: Naive Bayes\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=sUtPiyMnkIU&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's pre-class songs\n",
    "\n",
    "1. If It Weren't for Love (The Whistle Song) by José Lucas\n",
    "2. Animal Crossing by Shawn Wasabi, Sophie Black\n",
    "3. Pokémon Theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a2 posted\n",
    "- a1 solution posted (but I forgot to post the code, will do that soon)\n",
    "- Last class I forgot to mention I updated the Lecture 4 document with some new examples\n",
    "- Office hours queue: https://queue.students.cs.ubc.ca/course/17/\n",
    "- Tutorials\n",
    "  - Are completely optional\n",
    "  - We will standardize them more in future weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey\n",
    "\n",
    "https://ubc.ca1.qualtrics.com/jfe/form/SV_6YkzonFPmnDsD30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- k-NN recap\n",
    "- digit classification\n",
    "- data augmentation\n",
    "- bag of words\n",
    "- notation review\n",
    "- probabilistic classifiers\n",
    "- Bayesian classification\n",
    "- naive Bayes \n",
    "- Laplace smoothing\n",
    "- decision theory\n",
    "- decision trees vs. naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions megathread\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/214758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. In supervised learning, sometimes making up fake training data can be extremely useful.\n",
    "2. The \"Bag of Words\" representation of text disregards the ordering of words in a document.\n",
    "3. The naive Bayes assumption means we assume p(\"hello\"=1,\"vicodin\"=1,\"340\"=1) = p(\"hello\"=1)p(\"vicodin\"=1)p(\"340\"=1).\n",
    "4. Naive Bayes is limited to binary classification (that is, 2 possible values of y, like \"spam\" and \"not spam\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers**:\n",
    "\n",
    "1. True (data augmentation)\n",
    "2. Yes, at least for unigrams - bigrams/trigrams/etc don't _quite_ throw away all the ordering info.\n",
    "3. False. Should be The naive Bayes assumption means we assume p(\"hello\"=1,\"vicodin\"=1,\"340\"=1 | spam) = p(\"hello\"=1 | spam)p(\"vicodin\"=1 | spam)p(\"340\"=1 | spam).\n",
    "4. False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W2 midterm\n",
    "\n",
    "What is the space complexity of a trained naive Bayes model with binary features and binary labels? Answer using big-O notation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: What we have to store is, for each feature, 1 conditional probability for spam and one for not spam. Overall, we have a table of size $d \\times k$ where $k$ is the number of classes. In this case, $k=2$ (binary labels) so we are storing $2d$ conditional probabilities. We're also storing p(spam) and p(not spam) so in total we store $2d+k$ values. $O(kd)$ but since it's specified that $k=2$ we can say $O(d)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: for the space complexity question why is it 2d+k instead of 4d for spam and non-spam?\n",
    "A: If you store p(hello = 1 | spam) you don't need to separately store p(hello = 0 | spam) because p(hello = 1 | spam) + p(hello = 0 | spam) = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017W2 midterm\n",
    "\n",
    "What can go wrong is Laplace smoothing is not used with naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer**: If something never occurred in the training data, you'll assume it's probability is 0. E.g. if you never had the word \"hello\" in a spam message, you'll assume p(hello=1|spam)=0. Now you'll have a multiplication by 0 in your big product and so the whole thing will come out zero. Laplace smoothing solves this by making sure none of your estimated probabilities are zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions I just made up\n",
    "\n",
    "In naive Bayes, why is p(hello|spam)p(vicodin|spam)p(340|spam) easier to approximate than p(hello,vicodin,340|spam)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time permitting\n",
    "\n",
    "We can look at the \"naive Bayes by hand\" question in a2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

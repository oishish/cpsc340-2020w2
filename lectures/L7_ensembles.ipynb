{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 7: Ensemble Methods\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=3SD6fgNGZSo&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's pre-class songs\n",
    "\n",
    "1. Heybb! by binki\n",
    "2. Heaven by The Walkmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a2 posted\n",
    "- a1 solution updated to include code\n",
    "- Survey results: https://edstem.org/us/courses/3226/discussion/218421\n",
    "  - You seem to be OK with breakout rooms ✔️\n",
    "  - You seem to be OK with recording the chat ✔️\n",
    "  - Your preference for how we spend class time seems to be:\n",
    "  1. Old exam questions\n",
    "  2. Video summary\n",
    "  3. True/False polls\n",
    "  4. Answering student questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- decision theory\n",
    "- classification metrics (bonus)\n",
    "- prominent hyperparameters\n",
    "- naive Bayes recap\n",
    "- ensembles - intro\n",
    "- averaging models\n",
    "- random forests\n",
    "- part 1 recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2016W2 midterm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017W2 midterm\n",
    "\n",
    "Consider the classification boundary below, with the training data shown.\n",
    "\n",
    "![](img/L7/rf.png)\n",
    "\n",
    "Do you think this boundary was produced by a decision tree or a random forest? Briefly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 2017S1 midterm\n",
    "\n",
    "Why do we create random forests out of random trees rather than creating them out of regular decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "The following hyperparameter changes would result in **lower training error**:\n",
    "\n",
    "1. Increasing the number of features considered for each split in a random forest will tend to decrease training error.\n",
    "2. Increasing the number of trees in a random forest will tend to decrease training error.\n",
    "3. Let classifiers A, B, and C have training errors of 10%, 30%, and 30%, respectively. Then, the best possible training error from averaging $A$, $B$ and $C$ is 10%.\n",
    "4. Let classifiers A, B, and C all have 30% error. Then, the world possible training error from averaging $A$, $B$ and $C$ is 30%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 25b: optional review / more PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Last Of The Red Hot Fools by The Jitters\n",
    "2. Are You All Good? by breathe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- [Ed post on kernel k-means](https://edstem.org/us/courses/3226/discussion/327280)\n",
    "- Summer TA positions available (including CPSC 340 in summer term 1): http://cs.ubc.ca/ta\n",
    "- a5 opened for submissions of Gradescope\n",
    "- a5 due Wed 11:55pm\n",
    "- Amit not coming this week :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017W1 final\n",
    "\n",
    "What are two reasons that the minimizer $W$ of the PCA objective is not unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answer:** we can do a bunch of transformations that do not change the loss label switching, rotation, scaling. \n",
    "\n",
    "BTW \"the PCA objective\" here refers to $\\| X-ZW \\|^2_F$. When people say PCA the usually mean you're constraining $W$ to be an orthogonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that PCA and $k$-means are trying to optimize the same objective function (sum of squared errors). If we could find the optimal $k$-means clustering, why would it have a higher value of this objective function than PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: with $k$-means we also have a $Z$ and a $W$. In that case $W$ stores our $k$ cluster means. In PCA when you look at a row of $Z$, it's giving you a mixture of basis vectors. \n",
    "\n",
    "**Mike's answer:** We're minimizing the same loss but with _additional constraints_ for $k$-means, namely that each row of $Z$ has a 1 and the rest 0s. Adding these constraints is going to increase the loss.\n",
    "\n",
    "Note: we need to assume we're using the same $k$ for PCA and $k$-means in this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions\n",
    "\n",
    "1. It's best to pick $k$ as large as possible so that we capture most of the variance in the data.\n",
    "2. The first principal component is always the one with highest variance in the data.\n",
    "3. If your $X$ has two identical columns, then the variance explained by your last (i.e. $d$th) principal component will be zero.\n",
    "4. If your $X$ has two identical rows, then your $Z$ must also have two identical rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mike's answers:**\n",
    "\n",
    "1. False. Like $k$-means, we can always decrease the loss by increasing $k$, but maximizing $k$ isn't necessarily of interest. If we set $k=d$ (the maximum value of $k$) then we aren't reducing the dimension anymore.\n",
    "2. True\n",
    "3. True\n",
    "4. True. \n",
    "\n",
    "X has two identical rows -> Z has two identical rows\n",
    "\n",
    "Z has two identical rows -///NOT///-> X has two identical rows (we discussed this last time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That variance explained stuff\n",
    "\n",
    "- In PCA our loss is $f(Z,W)=\\|X-ZW\\|^2_F$. This is _not_ between 0 and 1 (it can be a huge number).\n",
    "- Variance explained is basically a rescaling of the loss.\n",
    "  - Higher is better (unlike loss)\n",
    "  - It's between $[0, 1]$\n",
    "  - When you have $k=0$ then variance explained is 0\n",
    "  - When you have $k=d$ then variance explained is 1\n",
    "  - It goes up as you increase $k$.\n",
    "- You might see \"variance remaining\" which would be 1 minus variance explained.\n",
    "- I haven't seen \"variance captured\" but I'd guess it's a synonym for variance explained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Q&A\n",
    "\n",
    "About PCA, earlier material, other machine learning, life..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

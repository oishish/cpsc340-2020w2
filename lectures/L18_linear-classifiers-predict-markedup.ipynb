{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 18: linear classifiers: `predict`\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=GMEDGjpJycA&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tender by Blur\n",
    "2. The Office theme song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 due March 10\n",
    "  - [Small typo](https://edstem.org/us/courses/3226/discussion/282829): in Q2.2 `minimizers.findMinL1` should say `findMin.findMinL1`\n",
    "- Midterm grading underway for written questions\n",
    "- MCQ average 76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm questions with the lowest scores\n",
    "\n",
    "1. What is the time complexity of predicting on one test example for regression with RBF features? (49% correct)\n",
    "\n",
    "Answer: $O(nd)$. You have your xtest vector which contains $d$ elements. You need to make ztest with will contain $n$ elements because you have $n$ of the RBF features. For each element you need to compute a distance between two examples which costs you $O(d)$, hence $O(nd)$ total.\n",
    "\n",
    "2. Which of the following is a valid approximation of $\\min(a,b)$ using log-sum-exp? (52% correct)\n",
    "\n",
    "$\\min(a,b)= -\\max(-a,-b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- recap, plan for this lecture\n",
    "- classification using regression?\n",
    "- decision boundaries\n",
    "- perceptron\n",
    "- exploring the coefficients\n",
    "- predicted probabilities\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier True/False questions\n",
    "\n",
    "1. Logistic regression will predict the positive class if the predicted probability of the positive class is greater than 0.5.\n",
    "2. For linear classifiers, the decision boundary (the boundary dividing the two classes) is a $d$-dimensional hyperplane.\n",
    "3. \"Linear classifier\" means directly applying least squares linear regression to problems where the $y_i$ values are either $-1$ or $+1$. **NOTE:** This question should go with Lecture 19, not Lecture 18.\n",
    "4. The coefficients of a linear classifier have a similar interpretation to that of linear regression: if coefficient $j$ is large, that means a change in feature $j$ has a large impact on the prediction.\n",
    "5. In a linear classifier, decreasing the magnitude of all the coefficients leads to predicted probabilities that are closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. *True*\n",
    "2. ~*True*~ **False** Would be a $d - 1$ dimensional plane ;)\n",
    "3. **Ans: False**\n",
    "4. *True*\n",
    "5. *True*\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice question (note: material covered in CPSC 330)\n",
    "\n",
    "Consider the spam filtering application with bag-of-words features that we discussed earlier in the course, except this time using a linear classifier instead of naive Bayes. \n",
    "\n",
    "1. How can we use our linear classifier to find the \"most spammy\" **word** in the dataset?\n",
    "2. How can we use our linear classifier to find the \"most spammy\" **email** in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1. Find the word corresponding to the largest $w_j$.\n",
    "2. Apply the model to all emails in the dataset and look for the email with the largest probability of spam.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPSC 330 2020W1 final\n",
    "\n",
    "If we've fit a linear classifier and we want to find the most important feature, would it make more sense to look at the largest coefficient in absolute value or just the largest (most positive) coefficient? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Largest abs value\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another practice question\n",
    "\n",
    "Consider the dataset\n",
    "\n",
    "```\n",
    "X = [1 2 3]   y = [+1]\n",
    "    [6 5 4]       [+1]\n",
    "    [7 8 9]       [-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a linear classifier and we find the following weights:\n",
    "\n",
    "```\n",
    "w = [-1]\n",
    "    [ 0]\n",
    "    [ 2]\n",
    "```\n",
    "\n",
    "What is the training error (fraction of predictions that are incorrect) of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "1/3\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick note\n",
    "\n",
    "For the first time here, we're going to make a distinction between _training error_ and _training loss_. The training error (fraction incorrect) is not smooth/continuous/differentiable with respect to $w$. Thus next class we're going to introduce a smoothed version of this that we minimize. We'll now have two metrics to look at, both of which are of interest to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/284735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 340 Lecture 18: linear classifiers: `predict`\n",
    "\n",
    "This notebook is for the in-class activities. It assumes you have already watched the [associated video](https://www.youtube.com/watch?v=GMEDGjpJycA&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**REMINDER TO START RECORDING**</font>\n",
    "\n",
    "Also, reminder to enable screen sharing for Participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-class music\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tender by Blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin\n",
    "\n",
    "- a4 due March 10\n",
    "  - [Small typo](https://edstem.org/us/courses/3226/discussion/282829): in Q2.2 `minimizers.findMinL1` should say `findMin.findMinL1`\n",
    "- Midterm grading underway for written questions\n",
    "- MCQ average 76%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video chapters\n",
    "\n",
    "- recap, plan for this lecture\n",
    "- classification using regression?\n",
    "- decision boundaries\n",
    "- perceptron\n",
    "- exploring the coefficients\n",
    "- predicted probabilities\n",
    "- summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm questions with the lowest scores\n",
    "\n",
    "1. What is the time complexity of predicting on one test example for regression with RBF features? (49% correct)\n",
    "\n",
    "2. Which of the following is a valid approximation of $\\min(a,b)$ using log-sum-exp? (52% correct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier True/False questions\n",
    "\n",
    "1. Logistic regression will predict the positive class if the predicted probability of the positive class is greater than 0.5.\n",
    "2. For linear classifiers, the decision boundary (the boundary dividing the two classes) is a $d$-dimensional hyperplane.\n",
    "3. \"Linear classifier\" means directly applying least squares linear regression to problems where the $y_i$ values are either $-1$ or $+1$.\n",
    "4. The coefficients of a linear classifier have a similar interpretation to that of linear regression: if coefficient $j$ is large, that means a change in feature $j$ has a large impact on the prediction.\n",
    "5. In a linear classifier, decreasing the magnitude of all the coefficients leads to predicted probabilities that are closer to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old exam questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPSC 330 2020W1 final\n",
    "\n",
    "If we've fit a linear classifier and we want to find the most important feature, would it make more sense to look at the largest coefficient in absolute value or just the largest (most positive) coefficient? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPSC 330 2020W1 midterm (adapted)\n",
    "\n",
    "Consider the spam filtering application with bag-of-words features that we discussed earlier in the course, except this time using a linear classifier instead of naive Bayes. What is the main problem with the following statement:\n",
    "\n",
    "\"The most spammy email in the dataset is the email corresponding to the largest weight of our linear classifier.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More practice questions\n",
    "\n",
    "Consider the dataset\n",
    "\n",
    "```\n",
    "X = [1 2 3]   y = [+1]\n",
    "    [6 5 4]       [+1]\n",
    "    [7 8 9]       [-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a linear classifier and we find the following weights:\n",
    "\n",
    "```\n",
    "w = [-1]\n",
    "    [ 0]\n",
    "    [ 2]\n",
    "```\n",
    "\n",
    "What is the training error of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick note\n",
    "\n",
    "For the first time here, we're going to make a distinction between _training error_ and _training loss_. The training error (fraction incorrect) is not smooth/continuous/differentiable with respect to $w$. Thus next class we're going to introduce a smoothed version of this that we minimize. We'll now have two metrics to look at, both of which are of interest to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student questions\n",
    "\n",
    "https://edstem.org/us/courses/3226/discussion/284735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
